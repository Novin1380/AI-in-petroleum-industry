{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29cd9575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting image loading and preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16800\\230287891.py:42: RuntimeWarning: invalid value encountered in divide\n",
      "  poro_array_normalized = poro_array / poro_array.max() if poro_array.max() > 0 else poro_array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished image loading and preprocessing for 756 samples.\n",
      "Shape of processed permeability maps: (756, 64, 64, 1)\n",
      "Shape of processed porosity maps: (756, 64, 64, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_filtered_by_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 79\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of processed porosity maps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mporosity_maps_np\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Now, we need to filter our numerical dataframe one more time\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# to ensure it only contains the sample numbers for which images were successfully loaded.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Use df_filtered_by_images from previous step.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Filter the numerical data to match the successfully loaded images\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m df_final_numerical \u001b[38;5;241m=\u001b[39m df_filtered_by_images[df_filtered_by_images[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(processed_sample_numbers)]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mShape of final numerical DataFrame after aligning with processed images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_final_numerical\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of unique samples in final numerical DataFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_final_numerical[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_filtered_by_images' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base path where your 'assets' folder is located.\n",
    "# Assuming 'assets' is in the same directory as your script or notebook.\n",
    "BASE_PATH = 'assets'\n",
    "PERM_FOLDER = os.path.join(BASE_PATH, 'permeability')\n",
    "PORO_FOLDER = os.path.join(BASE_PATH, 'porosity')\n",
    "\n",
    "# Define the range of sample numbers we will process for images.\n",
    "# Based on our previous discussion, porosity maps go up to 756.\n",
    "MIN_SAMPLE_NUM = 1\n",
    "MAX_SAMPLE_NUM = 756 # As confirmed, porosity maps go up to poro_map_0756\n",
    "\n",
    "permeability_maps = []\n",
    "porosity_maps = []\n",
    "processed_sample_numbers = [] # To keep track of samples successfully processed\n",
    "\n",
    "print(\"Starting image loading and preprocessing...\")\n",
    "\n",
    "for sample_num in range(MIN_SAMPLE_NUM, MAX_SAMPLE_NUM + 1):\n",
    "    perm_filename = os.path.join(PERM_FOLDER, f'perm_map_{sample_num:04d}.tiff')\n",
    "    poro_filename = os.path.join(PORO_FOLDER, f'poro_map_{sample_num:04d}.tiff')\n",
    "\n",
    "    try:\n",
    "        # Load permeability map\n",
    "        with Image.open(perm_filename) as perm_img:\n",
    "            perm_array = np.array(perm_img, dtype=np.float32)\n",
    "            # Normalize to [0, 1] range. Assuming max pixel value is known or it's implicitly handled.\n",
    "            # If data is already normalized or has a specific max value (e.g., 255 for 8-bit images), adjust accordingly.\n",
    "            # For scientific data, max_value might be higher or vary, so we'll normalize by its max value\n",
    "            # or a theoretical maximum if known. Here, assuming we need to normalize based on current max.\n",
    "            perm_array_normalized = perm_array / perm_array.max() if perm_array.max() > 0 else perm_array\n",
    "            permeability_maps.append(perm_array_normalized)\n",
    "\n",
    "        # Load porosity map\n",
    "        with Image.open(poro_filename) as poro_img:\n",
    "            poro_array = np.array(poro_img, dtype=np.float32)\n",
    "            # Normalize to [0, 1] range\n",
    "            poro_array_normalized = poro_array / poro_array.max() if poro_array.max() > 0 else poro_array\n",
    "            porosity_maps.append(poro_array_normalized)\n",
    "\n",
    "        processed_sample_numbers.append(sample_num)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Missing file for sample {sample_num}. Skipping.\")\n",
    "        # If a file is missing for a sample, we should not include this sample\n",
    "        # in our final image datasets to maintain consistency.\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {sample_num}: {e}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "# Convert lists of arrays to a single NumPy array\n",
    "# Add a channel dimension for CNN input (e.g., (N, H, W, C) where C=1 for grayscale images)\n",
    "permeability_maps_np = np.array(permeability_maps)[..., np.newaxis]\n",
    "porosity_maps_np = np.array(porosity_maps)[..., np.newaxis]\n",
    "\n",
    "print(f\"\\nFinished image loading and preprocessing for {len(processed_sample_numbers)} samples.\")\n",
    "print(f\"Shape of processed permeability maps: {permeability_maps_np.shape}\")\n",
    "print(f\"Shape of processed porosity maps: {porosity_maps_np.shape}\")\n",
    "\n",
    "# Now, we need to filter our numerical dataframe one more time\n",
    "# to ensure it only contains the sample numbers for which images were successfully loaded.\n",
    "# Use df_filtered_by_images from previous step.\n",
    "# Make sure df_filtered_by_images is defined, if not, re-run the previous cells or define it here for execution\n",
    "# For continuity, let's assume df_filtered_by_images is already defined from previous steps.\n",
    "\n",
    "# If df_filtered_by_images wasn't already available from the previous run,\n",
    "# uncomment and run these lines to ensure it's defined:\n",
    "# df = pd.read_csv('data.xlsx - Sheet1.csv')\n",
    "# df_cleaned = df.dropna(subset=['Initial Sw', 'Oil Rate (m3/day)', 'Cumulative Oil (M m3)'])\n",
    "# df_filtered_by_images = df_cleaned[df_cleaned['sample number'] <= MAX_SAMPLE_NUM].copy()\n",
    "\n",
    "\n",
    "# Filter the numerical data to match the successfully loaded images\n",
    "df_final_numerical = df_filtered_by_images[df_filtered_by_images['sample number'].isin(processed_sample_numbers)].copy()\n",
    "\n",
    "print(f\"\\nShape of final numerical DataFrame after aligning with processed images: {df_final_numerical.shape}\")\n",
    "print(f\"Number of unique samples in final numerical DataFrame: {df_final_numerical['sample number'].nunique()}\")\n",
    "print(\"First few rows of the final numerical DataFrame:\")\n",
    "print(df_final_numerical.head())\n",
    "\n",
    "# At this point, you have:\n",
    "# - permeability_maps_np: A NumPy array of all processed permeability maps.\n",
    "# - porosity_maps_np: A NumPy array of all processed porosity maps.\n",
    "# - df_final_numerical: A pandas DataFrame containing the numerical data (Initial Sw, Oil Rate, Cumulative Oil)\n",
    "#                       for exactly the same samples for which images were loaded.\n",
    "\n",
    "# The next step will be to prepare these data structures for input into a deep neural network,\n",
    "# including splitting them into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a7271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نوع داده‌ای پیکسل‌ها: float32\n",
      "کمترین مقدار پیکسل در تصویر نمونه: 0.003000000026077032\n",
      "بیشترین مقدار پیکسل در تصویر نمونه: 261.2349853515625\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "sample_image_path = './assets/permeability/perm_map_0001.tiff'\n",
    "img = Image.open(sample_image_path)\n",
    "img_array = np.array(img)\n",
    "\n",
    "print(f\"نوع داده‌ای پیکسل‌ها: {img_array.dtype}\")\n",
    "print(f\"کمترین مقدار پیکسل در تصویر نمونه: {img_array.min()}\")\n",
    "print(f\"بیشترین مقدار پیکسل در تصویر نمونه: {img_array.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966ff10c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_final_numerical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming permeability_maps_np, porosity_maps_np, and df_final_numerical are already defined from previous successful execution.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# If you are restarting the session, you might need to re-run the previous code cells to define these variables.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Sort the numerical dataframe by 'sample number' and 'Month' to ensure consistency\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m df_final_numerical \u001b[38;5;241m=\u001b[39m df_final_numerical\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth (2026)\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Extract Initial Sw values. Since Sw is per sample, and images are per sample,\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# we need to replicate Sw for each month's entry for that sample in the training/splitting process.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# However, a cleaner approach for multi-modal input is to pass Sw as a separate branch in the model.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# For splitting, we will use unique sample numbers.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Get unique sample numbers for splitting\u001b[39;00m\n\u001b[0;32m     21\u001b[0m all_sample_numbers \u001b[38;5;241m=\u001b[39m df_final_numerical[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_final_numerical' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming permeability_maps_np, porosity_maps_np, and df_final_numerical are already defined from previous successful execution.\n",
    "# If you are restarting the session, you might need to re-run the previous code cells to define these variables.\n",
    "\n",
    "# --- 1. Prepare Numerical Inputs and Targets ---\n",
    "# The numerical DataFrame (df_final_numerical) already contains Initial Sw, Oil Rate, and Cumulative Oil\n",
    "# aligned with the processed image sample numbers.\n",
    "\n",
    "# Sort the numerical dataframe by 'sample number' and 'Month' to ensure consistency\n",
    "df_final_numerical = df_final_numerical.sort_values(by=['sample number', 'Month (2026)']).reset_index(drop=True)\n",
    "\n",
    "# Extract Initial Sw values. Since Sw is per sample, and images are per sample,\n",
    "# we need to replicate Sw for each month's entry for that sample in the training/splitting process.\n",
    "# However, a cleaner approach for multi-modal input is to pass Sw as a separate branch in the model.\n",
    "# For splitting, we will use unique sample numbers.\n",
    "\n",
    "# Get unique sample numbers for splitting\n",
    "all_sample_numbers = df_final_numerical['sample number'].unique()\n",
    "\n",
    "# Separate the numerical input (Initial Sw) and the target variables\n",
    "# Note: Initial Sw is constant per sample, so we can extract it once per unique sample.\n",
    "# The `Month (2026)` column also serves as a numerical input.\n",
    "numerical_input_data = df_final_numerical[['Initial Sw', 'Month (2026)']].values\n",
    "target_oil_rate = df_final_numerical['Oil Rate (m3/day)'].values\n",
    "target_cumulative_oil = df_final_numerical['Cumulative Oil (M m3)'].values\n",
    "\n",
    "# --- 2. Splitting the Dataset (by Sample Number) ---\n",
    "# It's crucial to split based on unique sample numbers to prevent data leakage.\n",
    "# Data from all months for a given sample must go into the same split (train/val/test).\n",
    "\n",
    "# We will split the `all_sample_numbers` first, then use these lists of sample numbers\n",
    "# to filter the image arrays and the numerical DataFrame.\n",
    "\n",
    "# First, split into training and a temporary set (validation + test)\n",
    "train_samples, temp_samples = train_test_split(\n",
    "    all_sample_numbers,\n",
    "    test_size=0.3, # e.g., 70% train, 30% temp\n",
    "    random_state=42 # for reproducibility\n",
    ")\n",
    "\n",
    "# Then, split the temporary set into validation and test sets\n",
    "val_samples, test_samples = train_test_split(\n",
    "    temp_samples,\n",
    "    test_size=0.5, # 50% of temp, so 15% of total for validation and 15% for test\n",
    "    random_state=42 # for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal unique samples: {len(all_sample_numbers)}\")\n",
    "print(f\"Training samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")\n",
    "print(f\"Test samples: {len(test_samples)}\")\n",
    "\n",
    "# --- 3. Create Dictionaries/Mapping for Image Data ---\n",
    "# Map sample number to its index in the image arrays (0 to 755 for 756 samples)\n",
    "# This mapping assumes that images were loaded sequentially for sample numbers 1 to 756.\n",
    "# We need to map the actual sample number (e.g., 1) to its index in the np arrays (e.g., 0).\n",
    "# The 'processed_sample_numbers' list generated during image loading gives us this mapping.\n",
    "\n",
    "sample_num_to_img_idx = {num: idx for idx, num in enumerate(processed_sample_numbers)}\n",
    "\n",
    "# --- 4. Prepare Final Train/Validation/Test Datasets ---\n",
    "# Initialize lists to hold the data for each split\n",
    "X_train_perm, X_train_poro, X_train_sw_month = [], [], []\n",
    "y_train_oil_rate, y_train_cumulative_oil = [], []\n",
    "\n",
    "X_val_perm, X_val_poro, X_val_sw_month = [], [], []\n",
    "y_val_oil_rate, y_val_cumulative_oil = [], []\n",
    "\n",
    "X_test_perm, X_test_poro, X_test_sw_month = [], [], []\n",
    "y_test_oil_rate, y_test_cumulative_oil = [], []\n",
    "\n",
    "\n",
    "# Function to append data to lists for a given set of samples\n",
    "def populate_data_lists(sample_list, perm_list, poro_list, sw_month_list, oil_rate_list, cumulative_oil_list, df_data, img_perm_arr, img_poro_arr, mapping):\n",
    "    for sample in sample_list:\n",
    "        # Get the corresponding rows from the numerical DataFrame for this sample\n",
    "        sample_rows = df_data[df_data['sample number'] == sample]\n",
    "\n",
    "        # Get the image index for this sample\n",
    "        img_idx = mapping.get(sample)\n",
    "        if img_idx is None:\n",
    "            # This should ideally not happen if filtering was done correctly\n",
    "            # but acts as a safeguard.\n",
    "            print(f\"Error: Image index not found for sample {sample}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Append data for each month of this sample\n",
    "        for _, row in sample_rows.iterrows():\n",
    "            perm_list.append(img_perm_arr[img_idx])\n",
    "            poro_list.append(img_poro_arr[img_idx])\n",
    "            sw_month_list.append([row['Initial Sw'], row['Month (2026)']]) # Combine Sw and Month as a single numerical input for this entry\n",
    "            oil_rate_list.append(row['Oil Rate (m3/day)'])\n",
    "            cumulative_oil_list.append(row['Cumulative Oil (M m3)'])\n",
    "\n",
    "# Populate training data\n",
    "print(\"\\nPopulating training data...\")\n",
    "populate_data_lists(\n",
    "    train_samples,\n",
    "    X_train_perm, X_train_poro, X_train_sw_month,\n",
    "    y_train_oil_rate, y_train_cumulative_oil,\n",
    "    df_final_numerical, permeability_maps_np, porosity_maps_np, sample_num_to_img_idx\n",
    ")\n",
    "\n",
    "# Populate validation data\n",
    "print(\"Populating validation data...\")\n",
    "populate_data_lists(\n",
    "    val_samples,\n",
    "    X_val_perm, X_val_poro, X_val_sw_month,\n",
    "    y_val_oil_rate, y_val_cumulative_oil,\n",
    "    df_final_numerical, permeability_maps_np, porosity_maps_np, sample_num_to_img_idx\n",
    ")\n",
    "\n",
    "# Populate test data\n",
    "print(\"Populating test data...\")\n",
    "populate_data_lists(\n",
    "    test_samples,\n",
    "    X_test_perm, X_test_poro, X_test_sw_month,\n",
    "    y_test_oil_rate, y_test_cumulative_oil,\n",
    "    df_final_numerical, permeability_maps_np, porosity_maps_np, sample_num_to_img_idx\n",
    ")\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X_train_perm_np = np.array(X_train_perm)\n",
    "X_train_poro_np = np.array(X_train_poro)\n",
    "X_train_sw_month_np = np.array(X_train_sw_month)\n",
    "y_train_oil_rate_np = np.array(y_train_oil_rate)\n",
    "y_train_cumulative_oil_np = np.array(y_train_cumulative_oil)\n",
    "\n",
    "X_val_perm_np = np.array(X_val_perm)\n",
    "X_val_poro_np = np.array(X_val_poro)\n",
    "X_val_sw_month_np = np.array(X_val_sw_month)\n",
    "y_val_oil_rate_np = np.array(y_val_oil_rate)\n",
    "y_val_cumulative_oil_np = np.array(y_val_cumulative_oil)\n",
    "\n",
    "X_test_perm_np = np.array(X_test_perm)\n",
    "X_test_poro_np = np.array(X_test_poro)\n",
    "X_test_sw_month_np = np.array(X_test_sw_month)\n",
    "y_test_oil_rate_np = np.array(y_test_oil_rate)\n",
    "y_test_cumulative_oil_np = np.array(y_test_cumulative_oil)\n",
    "\n",
    "print(\"\\n--- Final Dataset Shapes ---\")\n",
    "print(f\"X_train_perm_np shape: {X_train_perm_np.shape}\")\n",
    "print(f\"X_train_poro_np shape: {X_train_poro_np.shape}\")\n",
    "print(f\"X_train_sw_month_np shape: {X_train_sw_month_np.shape}\")\n",
    "print(f\"y_train_oil_rate_np shape: {y_train_oil_rate_np.shape}\")\n",
    "print(f\"y_train_cumulative_oil_np shape: {y_train_cumulative_oil_np.shape}\")\n",
    "\n",
    "print(f\"\\nX_val_perm_np shape: {X_val_perm_np.shape}\")\n",
    "print(f\"X_val_poro_np shape: {X_val_poro_np.shape}\")\n",
    "print(f\"X_val_sw_month_np shape: {X_val_sw_month_np.shape}\")\n",
    "print(f\"y_val_oil_rate_np shape: {y_val_oil_rate_np.shape}\")\n",
    "print(f\"y_val_cumulative_oil_np shape: {y_val_cumulative_oil_np.shape}\")\n",
    "\n",
    "print(f\"\\nX_test_perm_np shape: {X_test_perm_np.shape}\")\n",
    "print(f\"X_test_poro_np shape: {X_test_poro_np.shape}\")\n",
    "print(f\"X_test_sw_month_np shape: {X_test_sw_month_np.shape}\")\n",
    "print(f\"y_test_oil_rate_np shape: {y_test_oil_rate_np.shape}\")\n",
    "print(f\"y_test_cumulative_oil_np shape: {y_test_cumulative_oil_np.shape}\")\n",
    "\n",
    "# At this point, your data is fully prepared and split into:\n",
    "# Training Data: (X_train_perm_np, X_train_poro_np, X_train_sw_month_np) for inputs\n",
    "#                (y_train_oil_rate_np, y_train_cumulative_oil_np) for targets\n",
    "# Validation Data: Similar arrays for validation\n",
    "# Test Data: Similar arrays for testing\n",
    "\n",
    "# The next step is to define and build the Deep Neural Network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e98c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 16 variables whereas the saved optimizer has 30 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m X_numerical_scaled_all \u001b[38;5;241m=\u001b[39m num_scaler\u001b[38;5;241m.\u001b[39mtransform(X_numerical_all)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 5. Scale image data (فرض بر اینه که image_data مرتب با دیتاسته)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m X_image_scaled_all \u001b[38;5;241m=\u001b[39m X_image\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (perm_max \u001b[38;5;241m-\u001b[39m perm_min) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     25\u001b[0m     X_image_scaled_all[:, :, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (X_image_scaled_all[:, :, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m perm_min) \u001b[38;5;241m/\u001b[39m (perm_max \u001b[38;5;241m-\u001b[39m perm_min)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_image' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "\n",
    "# 1. Load data\n",
    "data = pd.read_csv('processed_tabular_data.csv')\n",
    "X_numerical_all = data['Initial Sw'].values.reshape(-1, 1)\n",
    "sample_numbers = data['sample number'].values\n",
    "\n",
    "# 2. Load scalers and model\n",
    "num_scaler = joblib.load('num_scaler.gz')\n",
    "y_scaler = joblib.load('y_scaler.gz')\n",
    "model = keras.models.load_model('best_model.keras')\n",
    "\n",
    "# 3. Prepare image data (فرض بر اینه که X_image رو داری یا باید بسازی)\n",
    "# X_image = ...  # باید این رو طبق پروژه‌ات بسازی یا لود کنی\n",
    "\n",
    "# 4. Scale numerical data\n",
    "X_numerical_scaled_all = num_scaler.transform(X_numerical_all)\n",
    "\n",
    "# 5. Scale image data (فرض بر اینه که image_data مرتب با دیتاسته)\n",
    "X_image_scaled_all = X_image.copy()\n",
    "if (perm_max - perm_min) != 0:\n",
    "    X_image_scaled_all[:, :, :, 0] = (X_image_scaled_all[:, :, :, 0] - perm_min) / (perm_max - perm_min)\n",
    "if (poro_max - poro_min) != 0:\n",
    "    X_image_scaled_all[:, :, :, 1] = (X_image_scaled_all[:, :, :, 1] - poro_min) / (poro_max - poro_min)\n",
    "\n",
    "# 6. Predict\n",
    "predictions_scaled = model.predict({\n",
    "    'image_input': np.array(X_image_scaled_all),\n",
    "    'numerical_input': np.array(X_numerical_scaled_all)\n",
    "})\n",
    "\n",
    "# 7. Inverse scaling\n",
    "predictions_original = y_scaler.inverse_transform(predictions_scaled)\n",
    "\n",
    "# 8. Post-process negative values\n",
    "predictions_original[predictions_original < 0] = 0\n",
    "\n",
    "# 9. Save to DataFrame\n",
    "target_cols = ['param1', 'param2', ..., 'param12']  # اسامی ۱۲ پارامتر خروجی مدل\n",
    "df_predictions = pd.DataFrame(predictions_original, columns=target_cols)\n",
    "df_predictions.insert(0, 'sample number', sample_numbers)\n",
    "\n",
    "# 10. Save to Excel\n",
    "df_predictions.to_excel('full_dataset_predictions.xlsx', index=False)\n",
    "print(\"✅ Predictions saved to 'full_dataset_predictions.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaef28d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
